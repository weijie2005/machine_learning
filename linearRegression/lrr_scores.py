from sklearn.linear_model import LinearRegression
import joblib
import os


'''
线性回归模型
一、模型定义
    模型定义：线性回归模型是一种用于预测连续数值的监督学习模型。
    1.一元线性回归：只有一个输入特征x，用于预测一个连续数值y。
    2.多元线性回归：有多个输入特征x1, x2, ..., xn，用于预测一个连续数值y。

二、线性回归方程：
    模型假设：假设输入特征为x，目标变量为y，模型的预测值为y_pred。
    模型公式：y_pred = w1*x1 + w2*x2 + ... + wn*xn + b
    其中，w1, w2, ..., wn 是模型的系数（权重），b 是模型的截距（偏置项）。
    模型目标：通过最小化预测值与真实值之间的均方误差（MSE）来训练模型，即最小化：
    MSE = (1/n) * Σ(y_pred - y_true)^2
    其中，n 是样本数量，y_pred 是模型的预测值，y_true 是真实值。

三、什么是损失函数（Loss Function）
    误差定义：误差是模型预测值与真实值之间的差异,即y_pred - y_true
    损失函数定义：损失函数是一种用于衡量模型预测值与真实值之间差异的函数。   
    损失函数目标：通过最小化损失函数值，来优化模型的系数和截距，使模型预测值与真实值更接近。
    损失函数最小化：通过优化算法（如梯度下降），最小化损失函数，找到最优的模型系数和截距，使模型预测值与真实值的差异最小。

四、损失函数的种类与数学表达式
    1.误差平方和即最小二乘法（OLS）:预测值与真实值之差的平方和
        公式：
            OLS = Σ(y_pred - y_true)^2
        特点：
            是最小二乘法的损失函数，用于最小化预测值与真实值之间的误差平方和。
            是一个凸函数，全局最优解唯一。
        用途：
            线性回归中最常用的损失函数（普通最小二乘法 OLS 的核心）。

    2.均方误差（MSE-Mean Squared Error）:预测值与真实值之差的平方和的平均值
        损失函数公式：MSE = (1/n) * Σ(y_pred - y_true)^2
        其中，n 是样本数量，y_pred 是模型的预测值，y_true 是真实值。
        特点：
            平方项会放大较大误差，对异常值敏感（适合数据分布较为规整的场景）。
            函数是连续可导的，便于使用梯度下降等优化算法求解最优参数。
        用途：
            线性回归中最常用的损失函数。
       
    3.平均绝对误差（MAE-Mean Absolute Error）:预测值与真实值之差的绝对值的平均值
        损失函数公式：MAE = (1/n) * Σ|y_pred - y_true|
        其中，n 是样本数量，y_pred 是模型的预测值，y_true 是真实值。
        特点：
            对异常值不敏感（鲁棒性更强）。
            在误差为 0 处不可导，优化时可能存在收敛速度慢的问题
        用途：
            适合数据中存在较多异常值的场景

    4.均方根误差（RMSE-Root Mean Squared Error）:预测值与真实值之差的平方的平均值的平方根
        损失函数公式：RMSE = sqrt(MSE)
        其中，MSE 是均方误差。
        特点：
            对量纲与原始数据一致（如预测房价时，RMSE 单位为 “元”），更易解释
            同样对异常值敏感
        用途：
            评估模型性能时常用（比 MSE 更直观）

    5.Huber 损失（Robust Loss）:结合 MSE 和 MAE 的优点，在误差较小时用平方项，误差较大时用绝对值项
        损失函数公式：
            当 |y_pred - y_true| <= δ 时，损失为 (1/2) * (y_pred - y_true)^2
            当 |y_pred - y_true| > δ 时，损失为 δ * |y_pred - y_true| - (1/2) * δ^2
        其中，n 是样本数量，y_pred 是模型的预测值，y_true 是真实值，δ 是一个阈值。
        特点：
            对异常值不敏感（鲁棒性更强），在误差较小时用平方项，误差较大时用绝对值项，平衡了 MSE 和 MAE 的优点。            
        用途：
            适合数据中存在较多异常值的场景，需要平衡误差敏感度和优化效率的场景

五、数据类型
    1.标量（Scalar）：单个数值，如 3、-5.2、100 等。
    2.向量（Vector）：有序的数值列表，如 [1, 2, 3]、[-1.5, 0.5, 2.0] 等。
    3.矩阵（Matrix）：二维数组，如 [[1, 2, 3], [4, 5, 6]]、[[-1.5, 0.5, 2.0], [3.2, -2.1, 1.8]] 等。
    4.张量（Tensor）：多维数组，如 3D 张量、4D 张量等。

六、常用导数公式：加法、乘法、除法、链式法则
    导数公式：
        1.常数函数的导数：
            函数：y = C（C为常数）
            导数：y' = 0
        2.幂函数的导数：
            函数：y = x^n（n为实数）
            导数：y' = nx^(n-1)
        3.指数函数的导数：
            函数：y = e^x（自然底数）
            导数：y' = e^x
        4.对数函数的导数：
            函数：y = log_a(x)（a为大于0且不等于1的数）
            导数：y' = 1/(x*ln(a))
        5.指数函数的导数：
            函数：y = a^x（a为大于0的数）
            导数：y' = a^x * ln(a)

    导数四则运算：
        1.加法规则：(u+v)' = u' + v'
        2.乘法规则：(u.v)' = u'v + uv'
        3.除法规则：(u/v)' = (u'v - uv')/v^2
        4.链式法则：(g(f(x)))' = g'(f(x)) * f'(x)

七、偏导（Partial Derivative）
    定义：偏导是多变量函数中，对其中一个变量求导，其他变量保持不变的导数。
    公式：(df/dx) = lim(h->0) [(f(x+h) - f(x))/h]
    其中，df/dx 是函数 f 在变量 x 上的偏导，h 是一个小的变化量。
    用途：
        用于优化多变量函数，找到函数的局部最优解。
        在机器学习中，用于计算梯度，优化模型参数。
    示例，二元函数偏导：
        函数：f(x,y) = x^2 + 3xy + y^3
        对 x 求偏导固定y即y为常数：f_x = 2x + 3y     #(x^2的导数为2x,3xy中y为常数，导数为3y, y^3导数为0) 
        对 y 求偏导固定x即x为常数：f_y = 3x + 3y^2   #(x^2导数为0,3xy中x为常数，导数为3x,y^3导数为3y^2)

八、向量（Vector）
    定义：向量是有序的数值列表，用于表示空间中的点或方向。
    表示：通常用小写字母表示，如 a = [1, 2, 3]。
    向量的加减乘除
        1.向量的加减：
            向量加法：u + v = [u1 + v1, u2 + v2, ..., un + vn]
            向量减法：u - v = [u1 - v1, u2 - v2, ..., un - vn]
        2.向量的数乘：
            向量数乘：c * u = [c * u1, c * u2, ..., c * un]
        3.向量的点积（内积）：
            向量点积：u . v = u1 * v1 + u2 * v2 + ... + un * vn
        4.向量的叉积（外积）：
            向量叉积：u x v = [u2 * v3 - u3 * v2, u3 * v1 - u1 * v3, u1 * v2 - u2 * v1]
        5.向量矩阵置换：
            向量矩阵置换：u^T = [u1, u2, ..., un]
        6.向量的范数：
            向量范数L1绝对值求和：||u|| = |u1| + |u2| + ... + |un|
            向量范数L2平方和开根号：||u|| = sqrt(u1^2 + u2^2 + ... + un^2)

    用途：
        用于表示空间中的点或方向。
        在机器学习中，用于表示特征向量。

九、矩阵（Matrix）
    定义：矩阵是二维数组，用于表示多个向量的集合。
    表示：通常用大写字母表示，如 A = [[1, 2, 3], [4, 5, 6]]。
    1.矩阵的运算
        1.矩阵的加减：
            矩阵加法：A + B = [[a11 + b11, a12 + b12, ..., a1n + b1n], [a21 + b21, a22 + b22, ..., a2n + b2n], ..., [am1 + bm1, am2 + bm2, ..., amn + bmn]]
            矩阵减法：A - B = [[a11 - b11, a12 - b12, ..., a1n - b1n], [a21 - b21, a22 - b22, ..., a2n - b2n], ..., [am1 - bm1, am2 - bm2, ..., amn - bmn]]
        2.矩阵点积（矩阵乘法）：
            矩阵乘法：A * B = [[a11 * b11 + a12 * b21, a11 * b12 + a12 * b22, ..., a1n * bn1 + a1n * bn2], [a21 * b11 + a22 * b21, a21 * b12 + a22 * b22, ..., a2n * bn1 + a2n * bn2], ..., [am1 * b11 + am2 * b21, am1 * b12 + am2 * b22, ..., amn * bn1 + amn * bn2]]
                矩阵乘法满足结合律：A * (B * C) = (A * B) * C
                矩阵乘法不满足交换律：A * B ≠ B * A（除非 B 是方阵，且 A 是 B 的逆矩阵）
        3.矩阵点乘：
            矩阵点乘对位元素相乘：A * B = [[a11 * b11, a12 * b12, ..., a1n * b1n], [a21 * b21, a22 * b22, ..., a2n * b2n], ..., [am1 * bm1, am2 * bm2, ..., amn * bmn]]
    2.矩阵的类型：
        1.方阵（Square Matrix）：行数等于列数的矩阵，如 A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]。
        2.上三角矩阵（Upper Triangular Matrix）：主对角线以下的元素全为0，如 A = [[1, 2, 3], [0, 4, 5], [0, 0, 6]]。
        3.下三角矩阵（Lower Triangular Matrix）：主对角线以上的元素全为0，如 A = [[1, 0, 0], [2, 3, 0], [4, 5, 6]]。
        4.对角矩阵（Diagonal Matrix）：主对角线上的元素非0，其他元素全为0，如 A = [[1, 0, 0], [0, 2, 0], [0, 0, 3]]。
        5.对称矩阵（Symmetric Matrix）：转置矩阵等于原矩阵，如 A = [[1, 2, 3], [2, 4, 5], [3, 5, 6]]。
        6.单位矩阵（Identity Matrix）：主对角线上的元素全为1，其他元素全为0，如 A = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]。
        7.逆矩阵（Invertible Matrix）：存在一个矩阵 B，使得 A * B = B * A = I（单位矩阵）。
    3.矩阵的转置：
        矩阵转置：A^T = [[a11, a21, ..., am1], [a12, a22, ..., am2], ..., [a1n, a2n, ..., amn]]
        矩阵转置性质：
            1.(A^T)^T = A 
            2.A^T * B = B^T * A（矩阵乘法满足交换律）  
            3.A^T * (B^T) = (A * B)^T（矩阵乘法满足结合律）
            4.(A + B)^T=A^T + B^T （矩阵加法满足结合律）

十、求解损失函数的方法
    单变量损失函数-求导
    多变量损失函数-偏导
    1.正规方程（Normal Equation）：通过偏导和矩阵转置求解参数
        公式：w=(X^T * X)^-1 * X^T * y
        用途：直接求解线性回归模型的参数，无需迭代。
        注意：仅适用于方阵，且 X 必须满秩（列数等于样本数）。
    2.梯度下降（Gradient Descent）：
        公式：w = w - α * (X^T * (X * w - y))
        用途：迭代优化损失函数，找到模型参数的最优解。
        注意：需要设置学习率 α 和迭代次数。

'''
def scores_model(study_scores,model_path):
    #训练数据
    X_train = [[80,86], [82,80], [85,78], [90,90], [86,82],[82,90],[78,80],[92,94]]
    y_train = [84.2,80.6,80.1,90,83.2,87.6,79.4,93.4]

    #创建模型
    mymodel = LinearRegression()
    print("模型:",mymodel)
    
    #训练模型
    mymodel.fit(X_train, y_train)
    print("模型系数:",mymodel.coef_)
    print("模型截距:",mymodel.intercept_)    

    #预测
    predictions = mymodel.predict(study_scores)
    print("预测结果:",predictions)

    #保存模型
    joblib.dump(mymodel, model_path)

    return 


if __name__ == '__main__':
    # 获取当前脚本的绝对路径
    script_path = os.path.abspath(__file__)
    # 获取脚本所在的目录
    current_directory = os.path.dirname(script_path)

    # 构建模型文件的完整路径
    model_path = os.path.join(f'{current_directory}', 'models')

    if not os.path.exists(model_path):
        os.makedirs(model_path)
    
    # 构建模型文件的完整路径
    model_path = os.path.join(model_path, 'student_scores_model.pkl')

    #测试数据，并保存训练好的模型
    study_scores = [[90,80]]
    #scores_model(study_scores,model_path)

    # 加载保存的模型
    new_mymodel = joblib.load(model_path)

    # 使用模型进行预测
    new_scores = [[68,80]]
    new_predictions = new_mymodel.predict(new_scores)

    print("模型重新预测结果:",new_predictions)
    