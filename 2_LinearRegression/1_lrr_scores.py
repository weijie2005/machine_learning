from sklearn.linear_model import LinearRegression
import joblib
import os


'''
分类与回归的定义
    分类（Classification）和回归（Regression）是机器学习中两大基础预测任务，核心区别在于预测目标的类型不同：分类预测离散类别标签（如 “是 / 否”“猫 / 狗”），回归预测连续数值（如 “房价”“温度”）
    1.分类任务（Classification
        定义：给定输入数据，模型学习输入与离散类别标签之间的映射关系，输出数据所属的类别（非连续值）
        分类任务的模型：
            1.逻辑回归（Logistic Regression）：虽名为 “回归”，但本质是二分类模型（输出类别概率）；
            2.支持向量机（SVM）：分类任务中通过最大化间隔划分类别（可通过核函数处理非线性分类）；
            3.朴素贝叶斯（Naive Bayes）：基于贝叶斯定理的概率分类模型（适合文本分类、垃圾邮件检测）          
            4.k-最近邻（KNN）：基于实例的学习方法，通过计算输入实例与训练集中所有实例的距离，选择最近的k个实例进行投票，确定输入实例的类别。

    2.回归任务（Regression）
        定义：给定输入数据，模型学习输入与连续数值目标之间的映射关系，输出一个具体的数值（可在某一区间内取任意值）
        常用的回归模型：
            1.线性回归（Linear Regression）：拟合输入与输出的线性关系（输出连续值）；
            2.岭回归（Ridge Regression）、Lasso 回归：带正则化的线性回归（解决过拟合）；
            3.多项式回归（Polynomial Regression）：拟合非线性连续关系（如二次函数、三次函数）
      
    3.即支持分类又支回归任务的模型：可灵活切换的模型（通过调整输出层 / 损失函数）
        1.决策树、随机森林、梯度提升树（XGBoost、LightGBM）：
            分类：输出类别概率（损失函数用交叉熵）；
            回归：输出连续值（损失函数用 MSE）；
        2.神经网络：
            分类：输出层用softmax激活（多分类）或sigmoid激活（二分类），损失函数用交叉熵；
            回归：输出层无激活函数，损失函数用 MSE。

线性回归模型
一、模型定义
    模型定义：线性回归模型是一种用于预测连续数值的监督学习模型。
    1.一元线性回归：只有一个输入特征x，用于预测一个连续数值y。
    2.多元线性回归：有多个输入特征x1, x2, ..., xn，用于预测一个连续数值y。

二、线性回归方程：
    模型假设：假设输入特征为x，目标变量为y，模型的预测值为y_pred。
    模型公式：y_pred = w1*x1 + w2*x2 + ... + wn*xn + b
    其中，w1, w2, ..., wn 是模型的系数（权重），b 是模型的截距（偏置项）。
    模型目标：通过最小化预测值与真实值之间的均方误差（MSE）来训练模型，即最小化：
    MSE = (1/n) * Σ(y_pred - y_true)^2
    其中，n 是样本数量，y_pred 是模型的预测值，y_true 是真实值。

三、什么是损失函数（Loss Function）
    误差定义：误差是模型预测值与真实值之间的差异,即y_pred - y_true
    损失函数定义：损失函数是一种用于衡量模型预测值与真实值之间差异的函数。   
    损失函数目标：通过最小化损失函数值，来优化模型的系数和截距，使模型预测值与真实值更接近。
    损失函数最小化：通过优化算法（如梯度下降），最小化损失函数，找到最优的模型系数和截距，使模型预测值与真实值的差异最小。

四、损失函数的种类与数学表达式
    1.误差平方和即最小二乘法（OLS）:预测值与真实值之差的平方和
        公式：
            OLS = Σ(y_pred - y_true)^2
        特点：
            是最小二乘法的损失函数，用于最小化预测值与真实值之间的误差平方和。
            是一个凸函数，全局最优解唯一。
        用途：
            线性回归中最常用的损失函数（普通最小二乘法 OLS 的核心）。

    2.均方误差（MSE-Mean Squared Error）:预测值与真实值之差的平方和的平均值
        损失函数公式：MSE = (1/n) * Σ(y_pred - y_true)^2
        其中，n 是样本数量，y_pred 是模型的预测值，y_true 是真实值。
        特点：
            平方项会放大较大误差，对异常值敏感（适合数据分布较为规整的场景）。
            函数是连续可导的，便于使用梯度下降等优化算法求解最优参数。
        用途：
            线性回归中最常用的损失函数。
       
    3.平均绝对误差（MAE-Mean Absolute Error）:预测值与真实值之差的绝对值的平均值
        损失函数公式：MAE = (1/n) * Σ|y_pred - y_true|
        其中，n 是样本数量，y_pred 是模型的预测值，y_true 是真实值。
        特点：
            对异常值不敏感（鲁棒性更强）。
            在误差为 0 处不可导，优化时可能存在收敛速度慢的问题
        用途：
            适合数据中存在较多异常值的场景

    4.均方根误差（RMSE-Root Mean Squared Error）:预测值与真实值之差的平方的平均值的平方根
        损失函数公式：RMSE = sqrt(MSE)
        其中，MSE 是均方误差。
        特点：
            对量纲与原始数据一致（如预测房价时，RMSE 单位为 “元”），更易解释
            同样对异常值敏感
        用途：
            评估模型性能时常用（比 MSE 更直观）

    5.Huber 损失（Robust Loss）:结合 MSE 和 MAE 的优点，在误差较小时用平方项，误差较大时用绝对值项
        损失函数公式：
            当 |y_pred - y_true| <= δ 时，损失为 (1/2) * (y_pred - y_true)^2
            当 |y_pred - y_true| > δ 时，损失为 δ * |y_pred - y_true| - (1/2) * δ^2
        其中，n 是样本数量，y_pred 是模型的预测值，y_true 是真实值，δ 是一个阈值。
        特点：
            对异常值不敏感（鲁棒性更强），在误差较小时用平方项，误差较大时用绝对值项，平衡了 MSE 和 MAE 的优点。            
        用途：
            适合数据中存在较多异常值的场景，需要平衡误差敏感度和优化效率的场景

五、数据类型
    1.标量（Scalar）：0维数组，单个数值，如 3、-5.2、100 等。
    2.向量（Vector）：一维数组，有序的数值列表，如 [1, 2, 3]、[-1.5, 0.5, 2.0] 等。
    3.矩阵（Matrix）：二维数组，如 [[1, 2, 3], [4, 5, 6]]、[[-1.5, 0.5, 2.0], [3.2, -2.1, 1.8]] 等。
    4.张量（Tensor）：多维数组，如 3D 张量、4D 张量等。

六、常用导数公式：加法、乘法、除法、链式法则
    导数公式：
        1.常数函数的导数：
            函数：y = C（C为常数）
            导数：y' = 0
        2.幂函数的导数：
            函数：y = x^n（n为实数）
            导数：y' = nx^(n-1)
        3.指数函数的导数：
            函数：y = e^x（自然底数）
            导数：y' = e^x
        4.对数函数的导数：
            函数：y = log_a(x)（a为大于0且不等于1的数）
            导数：y' = 1/(x*ln(a))
        5.指数函数的导数：
            函数：y = a^x（a为大于0的数）
            导数：y' = a^x * ln(a)

    导数四则运算：
        1.加法规则：(u+v)' = u' + v'
        2.乘法规则：(u.v)' = u'v + uv'
        3.除法规则：(u/v)' = (u'v - uv')/v^2
        4.链式法则：(g(f(x)))' = g'(f(x)) * f'(x)

七、偏导（Partial Derivative）
    定义：偏导是多变量函数中，对其中一个变量求导，其他变量保持不变的导数。
    公式：(df/dx) = lim(h->0) [(f(x+h) - f(x))/h]
    其中，df/dx 是函数 f 在变量 x 上的偏导，h 是一个小的变化量。
    用途：
        用于优化多变量函数，找到函数的局部最优解。
        在机器学习中，用于计算梯度，优化模型参数。
    示例，二元函数偏导：
        函数：f(x,y) = x^2 + 3xy + y^3
        对 x 求偏导固定y即y为常数：f_x = 2x + 3y     #(x^2的导数为2x,3xy中y为常数，导数为3y, y^3导数为0) 
        对 y 求偏导固定x即x为常数：f_y = 3x + 3y^2   #(x^2导数为0,3xy中x为常数，导数为3x,y^3导数为3y^2)

八、向量（Vector）
    定义：向量是有序的数值列表，用于表示空间中的点或方向。
    表示：通常用小写字母表示，如 a = [1, 2, 3]。
    向量的加减乘除
        1.向量的加减：
            向量加法：u + v = [u1 + v1, u2 + v2, ..., un + vn]
            向量减法：u - v = [u1 - v1, u2 - v2, ..., un - vn]
        2.向量的数乘：
            向量数乘：c * u = [c * u1, c * u2, ..., c * un]
        3.向量的点积（内积）：
            向量点积：u . v = u1 * v1 + u2 * v2 + ... + un * vn
        4.向量的叉积（外积）：
            向量叉积：u x v = [u2 * v3 - u3 * v2, u3 * v1 - u1 * v3, u1 * v2 - u2 * v1]
        5.向量矩阵置换：
            向量矩阵置换：u^T = [u1, u2, ..., un]
        6.向量的范数：表示距离
            向量范数L1绝对值求和：||u|| = |u1| + |u2| + ... + |un|          曼哈顿距离，绝对值之和
            向量范数L2平方和开根号：||u|| = sqrt(u1^2 + u2^2 + ... + un^2)  欧式距离
            向量范数Lp：||u|| = (|u1|^p + |u2|^p + ... + |un|^p)^(1/p)     闵可夫斯基距离
            特性：向量u乘以u的转置：u^T * u = u的范数的平方

    用途：
        用于表示空间中的点或方向。
        在机器学习中，用于表示特征向量。

九、矩阵（Matrix）
    定义：矩阵是二维数组，用于表示多个向量的集合。
    表示：通常用大写字母表示，如 A = [[1, 2, 3], [4, 5, 6]]。
    1.矩阵的运算
        1.矩阵的加减：
            矩阵加法：A + B = [[a11 + b11, a12 + b12, ..., a1n + b1n], [a21 + b21, a22 + b22, ..., a2n + b2n], ..., [am1 + bm1, am2 + bm2, ..., amn + bmn]]
            矩阵减法：A - B = [[a11 - b11, a12 - b12, ..., a1n - b1n], [a21 - b21, a22 - b22, ..., a2n - b2n], ..., [am1 - bm1, am2 - bm2, ..., amn - bmn]]
        2.矩阵点积（矩阵乘法）：
            矩阵乘法：A * B = [[a11 * b11 + a12 * b21, a11 * b12 + a12 * b22, ..., a1n * bn1 + a1n * bn2], [a21 * b11 + a22 * b21, a21 * b12 + a22 * b22, ..., a2n * bn1 + a2n * bn2], ..., [am1 * b11 + am2 * b21, am1 * b12 + am2 * b22, ..., amn * bn1 + amn * bn2]]
                矩阵乘法满足结合律：A * (B * C) = (A * B) * C
                矩阵乘法不满足交换律：A * B ≠ B * A（除非 B 是方阵，且 A 是 B 的逆矩阵）
        3.矩阵点乘：
            矩阵点乘对位元素相乘：A * B = [[a11 * b11, a12 * b12, ..., a1n * b1n], [a21 * b21, a22 * b22, ..., a2n * b2n], ..., [am1 * bm1, am2 * bm2, ..., amn * bmn]]
    2.矩阵的类型：
        1.方阵（Square Matrix）：行数等于列数的矩阵，如 A = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]。
        2.上三角矩阵（Upper Triangular Matrix）：主对角线以下的元素全为0，如 A = [[1, 2, 3], [0, 4, 5], [0, 0, 6]]。
        3.下三角矩阵（Lower Triangular Matrix）：主对角线以上的元素全为0，如 A = [[1, 0, 0], [2, 3, 0], [4, 5, 6]]。
        4.对角矩阵（Diagonal Matrix）：主对角线上的元素非0，其他元素全为0，如 A = [[1, 0, 0], [0, 2, 0], [0, 0, 3]]。
        5.对称矩阵（Symmetric Matrix）：转置矩阵等于原矩阵，如 A = [[1, 2, 3], [2, 4, 5], [3, 5, 6]]。
        6.单位矩阵（Identity Matrix）：主对角线上的元素全为1，其他元素全为0，如 A = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]。
        7.逆矩阵（Invertible Matrix）：存在一个矩阵 B，使得 A * B = B * A = I（单位矩阵）。
    3.矩阵的转置：
        矩阵转置：A^T = [[a11, a21, ..., am1], [a12, a22, ..., am2], ..., [a1n, a2n, ..., amn]]
        矩阵转置性质：
            1.(A^T)^T = A 
            2.A^T * B = B^T * A（矩阵乘法满足交换律）  
            3.A^T * (B^T) = (A * B)^T（矩阵乘法满足结合律）
            4.(A + B)^T=A^T + B^T （矩阵加法满足结合律）

十、求解最优损失函数的方法
    单变量损失函数-斜率(导数)
    多变量损失函数-偏导
    1.正规方程（Normal Equation）：通过偏导和矩阵转置求解参数
        最小值公式：w=(X^T * X)^-1 * X^T * y    <=  f(x)=|| Xw-y ||^2 对w求最小值，<=使用该范数等于(A^T * A)的逆矩阵
        用途：直接求解线性回归模型的参数，无需迭代，特征数量较少时使用。
        注意：仅适用于方阵，且 X 必须满秩（列数等于样本数）,否则不可逆。所以正规方程有可能无法求解。

    2.梯度下降（Gradient Descent）：
        最小值公式：w = w - α * (X^T * (X * w - y))
        步长：α 是学习率，控制每次迭代的步长。
        迭代次数：需要根据实际情况设置迭代次数，确保模型收敛。
        用途：迭代优化损失函数，找到模型参数的最优解，完成收敛。适合特征数量较多的情况。
        注意：需要设置学习率 α 和迭代次数。        

十一、梯度下降算法的种类
        Gradient Descent（梯度下降）：GD
    1.全梯度下降（Full Gradient Descent）：FGD
        每次迭代使用所有样本计算梯度，更新参数。
        适用场景：样本数量较少时。

    2.随机梯度下降（Stochastic Gradient Descent）：SGD
        每次迭代随机选择一个样本计算梯度，更新参数。
        适用场景：样本数量较大时。

    3.小批量梯度下降（Mini-batch Gradient Descent）：MBGD
        每次迭代随机选择一小批量样本计算梯度，batch批次,batch_size每批次的大小
        适用场景：样本数量中等时。

    4.平均随机梯度下降（Stochastic Average Gradient Descent）：SAG
        每次迭代随机选择一个样本的梯度值和以往的梯度值的平均值，更新参数。
        适用场景：样本数量中等时。

十二、线性回归评估方法(与损失函数使用的方法相同)
    1.平均绝对误差MAE（Mean Absolute Error）：真实的误差
        公式：MAE = (1/n) * Σ|yi - ŷi|
        用途：评估模型预测值与真实值的平均绝对误差，越小越好。
        注意：对异常值不敏感，适合用于比较不同模型的性能。

    2.均方差MSE（Mean Squared Error）：
        公式：MSE = (1/n) * Σ(yi - ŷi)^2
        用途：评估模型预测值与真实值的平均平方误差，越小越好。
        注意：对异常值敏感，不适合用于比较不同模型的性能。

    3.均方根误差RMSE（Root Mean Squared Error）：异常值敏感，放大误差值，该值大说明异常值大
        公式：RMSE = sqrt(MSE)
        用途：评估模型预测值与真实值的平均平方根误差，越小越好。
        注意：对异常值敏感，不适合用于比较不同模型的性能。

十三、欠拟合与过拟合()的解决办法
    1.欠拟合解决办法：
        1.增加特征数量：通过添加更多的特征变量，提供更多的信息给模型。
        2.增加多项式特征：通过添加多项式特征，使模型能够更好地拟合数据。
        3.减少正则化参数：通过减少正则化参数，允许模型更复杂的拟合。
        4.使用更复杂的模型：尝试使用更复杂的模型，如多项式回归、支持向量机等。

    2.过拟合解决办法：
        1.减少特征数量：通过删除一些特征变量，减少模型的复杂度。特征降维
        2.增加样本数量：通过增加样本数量，提供更多的训练数据，帮助模型更好地学习。
        3.清洗数据：删除一些异常数据，如缺失值、异常值等。badcase坏数据或异常数据。

        4.增加正则化参数(L1,L2)：通过增加正则化参数，限制模型的参数值，防止过拟合。
        5.使用交叉验证：通过交叉验证，评估模型在不同数据集上的性能，选择最优模型。

十四、正则化（Regularization）
    1.正则化的作用：
        正则化是一种用于解决过拟合的技术，通过在损失函数中添加一个正则项来惩罚模型的复杂度。
        正则化项通常是模型参数的范数，如L1范数（Lasso回归）或L2范数（Ridge回归）。
        正则化可以将高次项系数限制在一个较小的范围内，减少过拟合。
    2.正则化的公式：
        L1正则化(Lasso回归)：损失函数 = 原始损失函数 + λ * Σ|wi| ,高次项系数变为0
        L2正则化(Ridge回归)：损失函数 = 原始损失函数 + λ * Σwi^2 ,高次项系数变成较小
    3.正则化的参数 λ（正则化强度）：
        λ 是一个超参数，用于控制正则化的强度。
        当 λ 较大时，正则化项的惩罚 stronger，模型的复杂度 lower。
        当 λ 较小时，正则化项的惩罚 weaker，模型的复杂度 higher。
    4.正则化的选择：
        L1正则化：适用于特征选择，将一些特征的系数设为0，实现特征的自动选择。
        L2正则化：适用于防止过拟合，减少模型的复杂度。

'''
def scores_model(study_scores,model_path):
    #训练数据
    X_train = [[80,86], [82,80], [85,78], [90,90], [86,82],[82,90],[78,80],[92,94]]
    y_train = [84.2,80.6,80.1,90,83.2,87.6,79.4,93.4]

    #创建模型
    mymodel = LinearRegression()
    print("模型:",mymodel)
    
    #训练模型
    mymodel.fit(X_train, y_train)
    print("模型系数:",mymodel.coef_)
    print("模型截距:",mymodel.intercept_)    

    #预测
    predictions = mymodel.predict(study_scores)
    print("预测结果:",predictions)

    #保存模型
    joblib.dump(mymodel, model_path)

    return 


if __name__ == '__main__':
    # 获取当前脚本的绝对路径
    script_path = os.path.abspath(__file__)
    # 获取脚本所在的目录
    current_directory = os.path.dirname(script_path)

    # 构建模型文件的完整路径
    model_path = os.path.join(f'{current_directory}', 'models')

    if not os.path.exists(model_path):
        os.makedirs(model_path)
    
    # 构建模型文件的完整路径
    model_path = os.path.join(model_path, 'student_scores_model.pkl')

    #测试数据，并保存训练好的模型
    study_scores = [[90,80]]
    #scores_model(study_scores,model_path)

    # 加载保存的模型
    new_mymodel = joblib.load(model_path)

    # 使用模型进行预测
    new_scores = [[68,80]]
    new_predictions = new_mymodel.predict(new_scores)

    print("模型重新预测结果:",new_predictions)
    